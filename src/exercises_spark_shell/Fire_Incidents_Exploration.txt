/*
import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.types.StructField
import org.apache.spark.sql.catalyst.ScalaReflection

//If I change the Timestamp for String, it works
case class Call(
Call_Number : Int,
Unit_ID : String,
Incident_Number : Int,
Call_Type : String,
Call_Date : java.sql.Timestamp,
Watch_Date : java.sql.Timestamp,
Received_DtTm : java.sql.Timestamp,
Entry_DtTm : java.sql.Timestamp,
Dispatch_DtTm : java.sql.Timestamp,
Response_DtTm : java.sql.Timestamp,
On_Scene_DtTm : java.sql.Timestamp,
Transport_DtTm : java.sql.Timestamp,
Hospital_DtTm : java.sql.Timestamp,
Call_Final_Disposition : String,
Available_DtTm : java.sql.Timestamp,
Address : String,
City : String,
Zipcode_of_Incident : Int,
Battalion : String,
Station_Area : Int,
Box : Int,
Original_Priority : Int,
Priority : Int,
Final_Priority : Int,
ALS_Unit : Boolean,
Call_Type_Group : String,
Number_of_Alarms : Int,
Unit_Type : String,
Unit_sequence_in_call_dispatch : Int,
Fire_Prevention_District : Int,
Supervisor_District : Int,
Neighborhood_District : String,
Location : String,
RowID : String
)
val schema = ScalaReflection.schemaFor[Call].dataType.asInstanceOf[StructType]
schema.printTreeString
*/
// Struct types directly
import org.apache.spark.sql.types.{ StructField, StructType, StringType, IntegerType, BooleanType }

val schema = StructType(Array(StructField("CallNumber", IntegerType, true),
                     StructField("UnitID", StringType, true),
                     StructField("IncidentNumber", IntegerType, true),
                     StructField("CallType", StringType, true),                  
                     StructField("CallDate", StringType, true),       
                     StructField("WatchDate", StringType, true),       
                     StructField("ReceivedDtTm", StringType, true),       
                     StructField("EntryDtTm", StringType, true),       
                     StructField("DispatchDtTm", StringType, true),       
                     StructField("ResponseDtTm", StringType, true),       
                     StructField("OnSceneDtTm", StringType, true),       
                     StructField("TransportDtTm", StringType, true),                  
                     StructField("HospitalDtTm", StringType, true),       
                     StructField("CallFinalDisposition", StringType, true),       
                     StructField("AvailableDtTm", StringType, true),       
                     StructField("Address", StringType, true),       
                     StructField("City", StringType, true),       
                     StructField("ZipcodeofIncident", IntegerType, true),       
                     StructField("Battalion", StringType, true),                 
                     StructField("StationArea", StringType, true),       
                     StructField("Box", StringType, true),       
                     StructField("OriginalPriority", StringType, true),       
                     StructField("Priority", StringType, true),       
                     StructField("FinalPriority", IntegerType, true),       
                     StructField("ALSUnit", BooleanType, true),       
                     StructField("CallTypeGroup", StringType, true),
                     StructField("NumberofAlarms", IntegerType, true),
                     StructField("UnitType", StringType, true),
                     StructField("Unitsequenceincalldispatch", IntegerType, true),
                     StructField("FirePreventionDistrict", StringType, true),
                     StructField("SupervisorDistrict", StringType, true),
                     StructField("NeighborhoodDistrict", StringType, true),
                     StructField("Location", StringType, true),
                     StructField("RowID", StringType, true)))
                     

// Read as dataframe
import org.apache.spark.sql.DataFrame

// We only have 4 cores, so 200 partitons is not something optimal as a core can proccess only one partiton at a time. It is better a 12 partitions,
// so every core executes only 3 partitions intead of 50 as it would be leaving the default parameter. 
spark.conf.get("spark.sql.shuffle.partitions")

spark.conf.set("spark.sql.shuffle.partitions", 12)

// The processor has only four cores(logical cores), and as the partiton should be <=200MB and a multiple of the cores (4), so 155MB es less than
// 200MB. Besides when 1804MB is divided by 155MB, it equals aproximately to 12, which is a multiple of 4
spark.conf.set("spark.sql.files.maxPartitionBytes", 155*1024*1024)

val fireDF: DataFrame = spark.read.format("csv").schema(schema).option("header", true).load("D:\\Fire_Department_Calls_for_Service.csv")

fireDF.rdd.getNumPartitions
fireDF.rdd.partitions.size


fireDF.take(10).foreach(println)

fireDF.printSchema()



//////////////////////////////Q-1) How many different types of calls were made to the Fire Department?
fireDF.select("CallType").show(5)

// Add the .distinct() transformation to keep only distinct rows
// The False below expands the ASCII column width to fit the full text in the output
fireDF.select("CallType").distinct().show(5, false)



//////////////////////////////Q-2) How many incidents of each call type were there?
fireDF.groupBy("CallType").count.sort($"count".desc).show



///////////////////////////////Q-3) How many years of Fire Service Calls is in the data file?
import org.apache.spark.sql.functions

val from_pattern1 = "MM/dd/yyyy"
val to_pattern1 = "yyyy-MM-dd"

val from_pattern2 = "MM/dd/yyyy hh:mm:ss aa"
val to_pattern2 = "MM/dd/yyyy hh:mm:ss aa"

// To introduce multiple lines of a command.
// :paste
val fireTsDF =  {
fireDF.withColumn("CallDateTS", unix_timestamp(fireDF("CallDate"), from_pattern1).cast("timestamp")).drop("CallDate") 
  .withColumn("WatchDateTS", unix_timestamp(fireDF("WatchDate"), from_pattern1).cast("timestamp")) 
  .drop("WatchDate") 
  .withColumn("ReceivedDtTmTS", unix_timestamp(fireDF("ReceivedDtTm"), from_pattern2).cast("timestamp")) 
  .drop("ReceivedDtTm") 
  .withColumn("EntryDtTmTS", unix_timestamp(fireDF("EntryDtTm"), from_pattern2).cast("timestamp")) 
  .drop("EntryDtTm") 
  .withColumn("DispatchDtTmTS", unix_timestamp(fireDF("DispatchDtTm"), from_pattern2).cast("timestamp")) 
  .drop("DispatchDtTm") 
  .withColumn("ResponseDtTmTS", unix_timestamp(fireDF("ResponseDtTm"), from_pattern2).cast("timestamp")) 
  .drop("ResponseDtTm") 
  .withColumn("OnSceneDtTmTS", unix_timestamp(fireDF("OnSceneDtTm"), from_pattern2).cast("timestamp")) 
  .drop("OnSceneDtTm") 
  .withColumn("TransportDtTmTS", unix_timestamp(fireDF("TransportDtTm"), from_pattern2).cast("timestamp")) 
  .drop("TransportDtTm") 
  .withColumn("HospitalDtTmTS", unix_timestamp(fireDF("HospitalDtTm"), from_pattern2).cast("timestamp")) 
  .drop("HospitalDtTm") 
  .withColumn("AvailableDtTmTS", unix_timestamp(fireDF("AvailableDtTm"), from_pattern2).cast("timestamp")) 
  .drop("AvailableDtTm")
} 


fireTsDF.select(year($"CallDateTS")).distinct().orderBy($"year(CallDateTS)".desc).show()

/////////////////////////////////Q-4) How many service calls were logged in the past 7 days?

fireTsDF.agg(max("CallDateTS")).show

fireTsDF.select("CallDateTS").orderBy($"CallDateTS".desc).show(10)

fireTsDF.select("CallDateTS").filter($"CallDateTS" > unix_timestamp(lit("2019-10-15"), "yyyy-MM-dd").cast("timestamp")).groupBy("CallDateTS").count().orderBy($"count".desc).show()



/////////////////////////  Memory, Caching and write to Parquet  ///////////////////////////////

//fireTsDF.rdd.getNumPartitions

// There is not limit of partitions?
//fireTsDF.repartition(6).createOrReplaceTempView("fireServiceVIEW");
fireTsDF.createOrReplaceTempView("fireServiceVIEW");

spark.catalog.cacheTable("fireServiceVIEW")

// Call .count() to materialize the cache
spark.table("fireServiceVIEW").count()

val fireServiceDF = spark.table("fireServiceVIEW")

// Note that the full scan + count in memory takes < 1 second!
fireServiceDF.count()

spark.catalog.isCached("fireServiceVIEW")

//%fs ls /tmp/

fireServiceDF.filter($"CallDateTS" > unix_timestamp(lit("2019-11-08"), "yyyy-MM-dd").cast("timestamp")).write.format("parquet").mode("overwrite").save("/tmp/fireServiceParquet/")

//%fs ls /tmp/fireServiceParquet/

val tempDF = spark.read.parquet("/tmp/fireServiceParquet/")

//import sys.process._
//"ls /tmp/fireServiceParquet/" !
//val sqlContext: SQLContext = new SQLContext(sc)

tempDF.limit(2)

spark.sql("""SELECT count(*) FROM fireServiceVIEW""").show

///////////////////////////////////Q-5) Which neighborhood in SF generated the most calls last year?

spark.sql("""SELECT NeighborhoodDistrict, count(NeighborhoodDistrict) AS Neighborhood_Count
				FROM fireServiceVIEW 
				WHERE year(CallDateTS) == '2015'
				GROUP BY NeighborhoodDistrict
				ORDER BY Neighborhood_Count DESC LIMIT 15
		  """).show

spark.sql("""SELECT NeighborhoodDistrict, count(NeighborhoodDistrict) AS Neighborhood_Count
				FROM fireServiceVIEW 
				WHERE year(CallDateTS) == '2015'
				GROUP BY NeighborhoodDistrict
				ORDER BY Neighborhood_Count DESC LIMIT 15
		  """).show


spark.sql("""DESC fireServiceVIEW
		  """).show


// Its posible to know the explain plan
spark.sql("""SELECT NeighborhoodDistrict, count(NeighborhoodDistrict) AS Neighborhood_Count
				FROM fireServiceVIEW 
				WHERE year(CallDateTS) == '2015'
				GROUP BY NeighborhoodDistrict
				ORDER BY Neighborhood_Count DESC LIMIT 15
		  """).explain(true)

//////////////////////////////Q-6) What was the primary non-medical reason most people called the fire department from the Tenderloin last year?

spark.conf.set("spark.sql.files.maxPartitionBytes", 45*1024*1024)

val incidentsDF = spark.read.option("header",true).option("inferSchema",true).csv(path="D:\\Fire_Incidents.csv").withColumnRenamed("Incident Number", "IncidentNumber")//.cache()

fireDF.rdd.getNumPartitions

incidentsDF.printSchema()

incidentsDF.count()

incidentsDF.limit(3).show

val joinedDF = fireServiceDF.join(incidentsDF, fireServiceDF("IncidentNumber") === incidentsDF("IncidentNumber"))

joinedDF.limit(3).show

joinedDF.count()

joinedDF.filter(year($"CallDateTS") === "2015").filter($"NeighborhoodDistrict" === "Tenderloin").count()


//////////////// Until here to avoid memory errors
joinedDF.filter(year($"CallDateTS") === "2015").filter($"NeighborhoodDistrict" === "Tenderloin").groupBy("Primary Situation").count().orderBy(desc("count")).limit(10).show

joinedDF.filter(year($"CallDateTS") === "2015").filter($"NeighborhoodDistrict" === "Russian Hill").groupBy("Primary Situation").count().orderBy(desc("count")).limit(10).show

